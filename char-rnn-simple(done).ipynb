{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/sjchoi86/advanced-tensorflow/blob/master/basic/char-rnn-simple.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIMPLE CHAR-RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENSORFLOW VERSION IS 1.0.1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "tf.set_random_seed(0)  \n",
    "print (\"TENSORFLOW VERSION IS %s\" % (tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINE TRAINING SEQUENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLLOWING IS OUR TRAINING SEQUENCE:\n",
      "Perfection is achieved, not when there is nothing more to add, but when there is nothing left to take away.\n"
     ]
    }
   ],
   "source": [
    "quote1 = (\"If you want to build a ship, \"\n",
    "          \"don't drum up people to collect wood and don't assign them tasks and work,\"\n",
    "          \" but rather teach them to long for the endless immensity of the sea.\")\n",
    "quote2 = (\"Perfection is achieved, \"\n",
    "          \"not when there is nothing more to add, \"\n",
    "          \"but when there is nothing left to take away.\")\n",
    "sentence = quote2\n",
    "print (\"FOLLOWING IS OUR TRAINING SEQUENCE:\")\n",
    "print (sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINE VOCABULARY AND DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCABULARY: \n",
      "[' ', ',', '.', 'P', 'a', 'c', 'b', 'e', 'd', 'g', 'f', 'i', 'h', 'k', 'm', 'l', 'o', 'n', 's', 'r', 'u', 't', 'w', 'v', 'y']\n",
      "DICTIONARY: \n",
      "{' ': 0, ',': 1, '.': 2, 'P': 3, 'a': 4, 'c': 5, 'b': 6, 'e': 7, 'd': 8, 'g': 9, 'f': 10, 'i': 11, 'h': 12, 'k': 13, 'm': 14, 'l': 15, 'o': 16, 'n': 17, 's': 18, 'r': 19, 'u': 20, 't': 21, 'w': 22, 'v': 23, 'y': 24}\n"
     ]
    }
   ],
   "source": [
    "char_set = list(set(sentence))\n",
    "char_dic = {w: i for i, w in enumerate(char_set)}\n",
    "print (\"VOCABULARY: \")\n",
    "print (char_set)\n",
    "print (\"DICTIONARY: \")\n",
    "print (char_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CONFIGURE NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dim    = len(char_set)\n",
    "num_classes = len(char_set)\n",
    "hidden_size     = 64\n",
    "sequence_length = 15  # Any arbitrary number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SET TRAINING BATCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0/ 107] [Perfection is a]=>[erfection is ac]\n",
      "            [3, 7, 19, 10, 7, 5, 21, 11, 16, 17, 0, 11, 18, 0, 4]=>[7, 19, 10, 7, 5, 21, 11, 16, 17, 0, 11, 18, 0, 4, 5]\n",
      "[   1/ 107] [erfection is ac]=>[rfection is ach]\n",
      "            [7, 19, 10, 7, 5, 21, 11, 16, 17, 0, 11, 18, 0, 4, 5]=>[19, 10, 7, 5, 21, 11, 16, 17, 0, 11, 18, 0, 4, 5, 12]\n",
      "[   2/ 107] [rfection is ach]=>[fection is achi]\n",
      "            [19, 10, 7, 5, 21, 11, 16, 17, 0, 11, 18, 0, 4, 5, 12]=>[10, 7, 5, 21, 11, 16, 17, 0, 11, 18, 0, 4, 5, 12, 11]\n",
      "[   3/ 107] [fection is achi]=>[ection is achie]\n",
      "            [10, 7, 5, 21, 11, 16, 17, 0, 11, 18, 0, 4, 5, 12, 11]=>[7, 5, 21, 11, 16, 17, 0, 11, 18, 0, 4, 5, 12, 11, 7]\n",
      "[   4/ 107] [ection is achie]=>[ction is achiev]\n",
      "            [7, 5, 21, 11, 16, 17, 0, 11, 18, 0, 4, 5, 12, 11, 7]=>[5, 21, 11, 16, 17, 0, 11, 18, 0, 4, 5, 12, 11, 7, 23]\n",
      "[   5/ 107] [ction is achiev]=>[tion is achieve]\n",
      "            [5, 21, 11, 16, 17, 0, 11, 18, 0, 4, 5, 12, 11, 7, 23]=>[21, 11, 16, 17, 0, 11, 18, 0, 4, 5, 12, 11, 7, 23, 7]\n",
      "[   6/ 107] [tion is achieve]=>[ion is achieved]\n",
      "            [21, 11, 16, 17, 0, 11, 18, 0, 4, 5, 12, 11, 7, 23, 7]=>[11, 16, 17, 0, 11, 18, 0, 4, 5, 12, 11, 7, 23, 7, 8]\n",
      "[   7/ 107] [ion is achieved]=>[on is achieved,]\n",
      "            [11, 16, 17, 0, 11, 18, 0, 4, 5, 12, 11, 7, 23, 7, 8]=>[16, 17, 0, 11, 18, 0, 4, 5, 12, 11, 7, 23, 7, 8, 1]\n",
      "[   8/ 107] [on is achieved,]=>[n is achieved, ]\n",
      "            [16, 17, 0, 11, 18, 0, 4, 5, 12, 11, 7, 23, 7, 8, 1]=>[17, 0, 11, 18, 0, 4, 5, 12, 11, 7, 23, 7, 8, 1, 0]\n",
      "[   9/ 107] [n is achieved, ]=>[ is achieved, n]\n",
      "            [17, 0, 11, 18, 0, 4, 5, 12, 11, 7, 23, 7, 8, 1, 0]=>[0, 11, 18, 0, 4, 5, 12, 11, 7, 23, 7, 8, 1, 0, 17]\n",
      "[  10/ 107] [ is achieved, n]=>[is achieved, no]\n",
      "            [0, 11, 18, 0, 4, 5, 12, 11, 7, 23, 7, 8, 1, 0, 17]=>[11, 18, 0, 4, 5, 12, 11, 7, 23, 7, 8, 1, 0, 17, 16]\n",
      "[  11/ 107] [is achieved, no]=>[s achieved, not]\n",
      "            [11, 18, 0, 4, 5, 12, 11, 7, 23, 7, 8, 1, 0, 17, 16]=>[18, 0, 4, 5, 12, 11, 7, 23, 7, 8, 1, 0, 17, 16, 21]\n",
      "[  12/ 107] [s achieved, not]=>[ achieved, not ]\n",
      "            [18, 0, 4, 5, 12, 11, 7, 23, 7, 8, 1, 0, 17, 16, 21]=>[0, 4, 5, 12, 11, 7, 23, 7, 8, 1, 0, 17, 16, 21, 0]\n",
      "[  13/ 107] [ achieved, not ]=>[achieved, not w]\n",
      "            [0, 4, 5, 12, 11, 7, 23, 7, 8, 1, 0, 17, 16, 21, 0]=>[4, 5, 12, 11, 7, 23, 7, 8, 1, 0, 17, 16, 21, 0, 22]\n",
      "[  14/ 107] [achieved, not w]=>[chieved, not wh]\n",
      "            [4, 5, 12, 11, 7, 23, 7, 8, 1, 0, 17, 16, 21, 0, 22]=>[5, 12, 11, 7, 23, 7, 8, 1, 0, 17, 16, 21, 0, 22, 12]\n"
     ]
    }
   ],
   "source": [
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i:i + sequence_length]\n",
    "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
    "    x = [char_dic[c] for c in x_str]  # x str to index\n",
    "    y = [char_dic[c] for c in y_str]  # y str to index\n",
    "    dataX.append(x)\n",
    "    dataY.append(y)\n",
    "    if i < 15:\n",
    "        print (\"[%4d/%4d] [%s]=>[%s]\" % (i, len(sentence), x_str, y_str))\n",
    "        print (\"%s%s=>%s\" % (' '*12, x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     'NDATA' IS 92\n",
      "'BATCH_SIZE' IS 512\n"
     ]
    }
   ],
   "source": [
    "ndata      = len(dataX)\n",
    "batch_size = 512\n",
    "print (\"     'NDATA' IS %d\" % (ndata))\n",
    "print (\"'BATCH_SIZE' IS %d\" % (batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINE PLACEHOLDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'sequence_length' IS [15]\n",
      "    'num_classes' IS [25]\n",
      "'X' LOOKS LIKE \n",
      "   [Tensor(\"Placeholder:0\", shape=(?, 15), dtype=int32)]\n",
      "'X_OH' LOOKS LIKE \n",
      "   [Tensor(\"one_hot:0\", shape=(?, 15, 25), dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length])\n",
    "X_OH = tf.one_hot(X, num_classes)\n",
    "print (\"'sequence_length' IS [%d]\" % (sequence_length))\n",
    "print (\"    'num_classes' IS [%d]\" % (num_classes))\n",
    "print(\"'X' LOOKS LIKE \\n   [%s]\" % (X))  \n",
    "print(\"'X_OH' LOOKS LIKE \\n   [%s]\" % (X_OH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUTS LOOKS LIKE [Tensor(\"CHAR-RNN/Reshape:0\", shape=(512, 15, 25), dtype=float32)]\n",
      "MODEL DEFINED.\n"
     ]
    }
   ],
   "source": [
    "num_hidden = 128\n",
    "with tf.variable_scope('CHAR-RNN', reuse=False):\n",
    "    cell = rnn.BasicLSTMCell(hidden_size, state_is_tuple=True)\n",
    "    cell = rnn.MultiRNNCell([cell] * 2, state_is_tuple=True)\n",
    "    # DYNAMIC RNN WITH FULLY CONNECTED LAYER\n",
    "    _hiddens  = tf.contrib.layers.fully_connected(X_OH, num_hidden, activation_fn=tf.nn.relu)\n",
    "    _outputs, _states = tf.nn.dynamic_rnn(cell, _hiddens, dtype=tf.float32)\n",
    "    _outputs  = tf.contrib.layers.fully_connected(_outputs, num_classes, activation_fn=None)\n",
    "    # RESHAPE FOR SEQUNCE LOSS\n",
    "    outputs = tf.reshape(_outputs, [batch_size, sequence_length, num_classes])\n",
    "print (\"OUTPUTS LOOKS LIKE [%s]\" % (outputs))\n",
    "print (\"MODEL DEFINED.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINE TF FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUNCTIONS DEFINED.\n"
     ]
    }
   ],
   "source": [
    "weights = tf.ones([batch_size, sequence_length]) # EQUAL WEIGHTS\n",
    "seq_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "    logits=outputs, targets=Y, weights=weights)\n",
    "loss  = tf.reduce_mean(seq_loss)\n",
    "optm  = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\n",
    "print (\"FUNCTIONS DEFINED.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIMIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0] loss_val: 3.21298 \n",
      "[  200] loss_val: 0.11954 \n",
      "[  400] loss_val: 0.11443 \n",
      "[  600] loss_val: 0.11208 \n",
      "[  800] loss_val: 0.11211 \n",
      "[ 1000] loss_val: 0.10972 \n",
      "[ 1200] loss_val: 0.10929 \n",
      "[ 1400] loss_val: 0.11141 \n",
      "[ 1600] loss_val: 0.11319 \n",
      "[ 1800] loss_val: 0.11087 \n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "MAXITER = 2000\n",
    "for i in range(MAXITER):\n",
    "    randidx = np.random.randint(low=0, high=ndata, size=batch_size)\n",
    "    batchX = [dataX[iii] for iii in randidx]\n",
    "    batchY = [dataY[iii] for iii in randidx]\n",
    "    feeds = {X: batchX, Y: batchY}\n",
    "    _, loss_val, results = sess.run(\n",
    "        [optm, loss, outputs], feed_dict=feeds)\n",
    "    if (i%200) == 0:\n",
    "        print (\"[%5d] loss_val: %.5f \" % (i, loss_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRINT CHARS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 'e', 'c', 't', 'i', 'o', 'n', ' ', 'i', 's', ' ', 'a', 'c', 'h', 'i']\n",
      "[' ', 't', 'h', 'i', 'n', 'g', ' ', 'l', 'o', 'r', 'e', ' ', 't', 'o', ' ']\n",
      "[' ', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'i', 's', ' ', 'n', 'o', 't', 'h']\n",
      "['s', 'n', ' ', 'i', 's', ' ', 'a', 'c', 'h', 'i', 'e', 'v', 'e', 'd', ',']\n",
      "['e', 'r', 'f', 'e', 'c', 't', 'i', 'o', 'n', ' ', 'i', 's', ' ', 'a', 'c']\n",
      "['e', 'r', 'e', ' ', 'i', 's', ' ', 'n', 'o', 't', 'h', 'i', 'n', 'g', ' ']\n",
      "[' ', 'e', 'd', ',', ' ', 'n', 'o', 't', ' ', 'w', 'h', 'e', 'n', ' ', 't']\n",
      "['t', 'i', 'o', 'n', ' ', 'i', 's', ' ', 'a', 'c', 'h', 'i', 'e', 'v', 'e']\n",
      "['h', 'i', 'r', 'e', ' ', 'i', 's', ' ', 'n', 'o', 't', 'h', 'i', 'n', 'g']\n",
      "['t', ' ', 'w', 'h', 'e', 'n', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'i', 's']\n"
     ]
    }
   ],
   "source": [
    "randidx = np.random.randint(low=0, high=ndata, size=batch_size)\n",
    "batchX = [dataX[iii] for iii in randidx]\n",
    "batchY = [dataY[iii] for iii in randidx]\n",
    "feeds = {X: batchX, Y: batchY}\n",
    "results = sess.run(outputs, feed_dict=feeds)\n",
    "for j, result in enumerate(results):\n",
    "    index = np.argmax(result, axis=1)\n",
    "    chars = [char_set[t] for t in index]\n",
    "    if j < 10:\n",
    "        print (chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAMPLING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_2:0\", shape=(?, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "LEN = 1;\n",
    "# XL = tf.placeholder(tf.int32, [None, LEN])\n",
    "XL     = tf.placeholder(tf.int32, [None, 1])\n",
    "XL_OH  = tf.one_hot(XL, num_classes)\n",
    "with tf.variable_scope('CHAR-RNN', reuse=True):\n",
    "    cell_L = rnn.BasicLSTMCell(hidden_size, state_is_tuple=True)\n",
    "    cell_L = rnn.MultiRNNCell([cell_L] * 2, state_is_tuple=True)\n",
    "    istate = cell_L.zero_state(batch_size=1, dtype=tf.float32)\n",
    "    # DYNAMIC RNN WITH FULLY CONNECTED LAYER\n",
    "    _hiddens  = tf.contrib.layers.fully_connected(XL_OH, num_hidden, activation_fn=tf.nn.tanh)\n",
    "    _outputs_L, states_L = tf.nn.dynamic_rnn(cell_L, _hiddens\n",
    "                                , initial_state=istate, dtype=tf.float32)\n",
    "    _outputs_L  = tf.contrib.layers.fully_connected(\n",
    "        _outputs_L, num_classes, activation_fn=None)\n",
    "    # RESHAPE FOR SEQUNCE LOSS\n",
    "    outputs_L = tf.reshape(_outputs_L, [LEN, 1, num_classes])\n",
    "print (XL)\n",
    "\n",
    "def weighted_pick(weights):\n",
    "    t = np.cumsum(weights)\n",
    "    s = np.sum(weights)\n",
    "    return(int(np.searchsorted(t, np.random.rand(1)*s)))\n",
    "def softmax(x):\n",
    "    alpha = 1\n",
    "    e_x = np.exp(alpha*(x - np.max(x)))\n",
    "    return e_x / np.sum(e_x) # only difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BURNIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prime = \"Perfection is\"\n",
    "istateval = sess.run(cell_L.zero_state(1, tf.float32))\n",
    "for c in prime[:-1]:\n",
    "    index = char_dic[c]\n",
    "    inval = [[index]]\n",
    "    outval, stateval = sess.run([outputs_L, states_L]\n",
    "                        , feed_dict={XL:inval, istate:istateval})\n",
    "    istateval = stateval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SAMPLED SETENCE> \n",
      " Perfection is achieved, nothing left to take away. more to add, but when there is nothing more to add, but when there is nothing more to add, but when there is noth\n",
      "\n",
      "<ORIGINAL SENTENCE> \n",
      " Perfection is achieved, not when there is nothing more to add, but when there is nothing left to take away.\n"
     ]
    }
   ],
   "source": [
    "inval  = [[char_dic[prime[-1]]]]\n",
    "outval, stateval = sess.run([outputs_L, states_L]\n",
    "                    , feed_dict={XL:inval, istate:istateval})\n",
    "istateval = stateval\n",
    "index = np.argmax(outval)\n",
    "char  = char_set[index]\n",
    "chars = char\n",
    "for i in range(150):\n",
    "    inval = [[index]]\n",
    "    outval, stateval = sess.run([outputs_L, states_L]\n",
    "                        , feed_dict={XL:inval, istate:istateval})\n",
    "    istateval = stateval\n",
    "    # index = np.argmax(outval)\n",
    "    index = weighted_pick(softmax(outval))\n",
    "    char  = char_set[index]\n",
    "    chars += char\n",
    "print (\"<SAMPLED SETENCE> \\n %s\" % (prime+chars))\n",
    "print (\"\\n<ORIGINAL SENTENCE> \\n %s\" % (sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
